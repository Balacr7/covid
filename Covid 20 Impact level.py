# -*- coding: utf-8 -*-
"""Final Merge-DSE_6000_SemProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13pf_UfKYYu4q5v-X1nJas6LNgixHdPHM
"""

pip install -U plotly

pip install bar_chart_race

pip install altair vega_datasets

import pandas as pd

df1 = pd.read_csv("https://storage.googleapis.com/analytics3/time_series_covid19_confirmed_US.csv")
df1.head(5)

pd.set_option('display.max_rows', 5000)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 5000)

df1.columns = df1.columns.str.lower() #Change all the column names to lowercase
df1.columns = df1.columns.str.replace('long_', 'long') #Change column name from "long_" to "long"
df1.drop(['iso2',"iso3", "code3", "country_region"] , axis=1 , inplace=True) #To the Copied data frame drop all columns other than date values
df1.head(4)

df1.dropna(subset=["fips"], inplace=True) #Drop null values in fips, county code cannot be NULL
df1['fips'] = df1['fips'].astype(int).astype(str) #Convert fips datatype from float to string
df1['fips'] = df1['fips'].map(lambda x: f'{x:0>5}') #Append 0 in the front of fips column to have length of 5
df1.head(4)

dates = df1.columns[7:]
confirmed_df1 = df1.melt(
    id_vars=['province_state', 'lat', 'long'], 
    value_vars=dates, 
    var_name='date', 
    value_name='confirmed'
)
confirmed_df1['date'] = pd.to_datetime(confirmed_df1['date'], format='%m/%d/%y')
confirmed_grp_df1 = confirmed_df1.groupby(['province_state']).sum('confirmed').reset_index()
confirmed_grp_df1.head(5)

state_df1 = confirmed_df1[confirmed_df1['province_state'].isin(['Florida', 'Texas'])]
state_df1.head()

df1_date = df1.copy(deep=True) #Copy the data frame to another dataframe to sum all the date values
df1_date.drop(["uid", "fips", "admin2", "province_state", 
                               "lat", "long", "combined_key"] , axis=1 , inplace=True) #To the Copied data frame drop all columns other than date values
df1['confirmed_covid'] = df1_date.sum(axis = 1, skipna = True) #Sum all the date column values and add it to the initial dataframe as a new column
df1[ (df1.fips == '06001')] #Check value for a county in California

"""Which state has got the highest per day count?"""

import seaborn as sns

sns.set(rc={'figure.figsize':(10,7)})
ax = sns.catplot(data=confirmed_df1, x='province_state', y='confirmed', height=9, aspect=8/4)
ax.set_xticklabels(rotation=90, fontsize = 10)
ax.set(xlabel='US States', ylabel='Confirmed/day')
ax.set(title="USA per day Confirmed count") 
ax.set(ylim=(0, None))

"""California has got the Highest per day count, following Illinois. Arizona and Texas almost in same level with per day count

How Covid-19 has affected the USA states?
"""

import bar_chart_race as bcr

df11 = df1.copy(deep=True) #Copy the data frame to another dataframe to sum all the date values

df11.drop(["uid", "fips", "admin2", "lat", "long", 
          "combined_key"] , axis=1 , inplace=True) #To the Copied data frame drop all columns other than date values

cols=df11.filter(regex=':*/20').columns #Fetch all the date column names

df12 = df11.melt(id_vars=['province_state'], value_vars=(cols), value_name='count', var_name='date') #Convert date columns to rows
df13 = df12.groupby(['province_state', 'date']).sum('count').reset_index() #Group by state and date to sum the death counts for each state/date
df13['date'] = pd.to_datetime(df13['date'], format='%m/%d/%y') #Change the date values to date format
df14 = df13.pivot_table(index=['date'], columns=['province_state']) #Convert province_state as rows to columns

bcr.bar_chart_race(
    df=df14,
    filename=None,
    orientation='h',
    sort='desc',
    n_bars=6,
    fixed_order=False,
    fixed_max=True,
    steps_per_period=10,
    interpolate_period=False,
    label_bars=True,
    bar_size=.95,
    period_label={'x': .99, 'y': .25, 'ha': 'right', 'va': 'center'},
    period_fmt='%B %d, %Y',
    period_summary_func=lambda v, r: {'x': .99, 'y': .18,
                                      's': f'Total Confirmed cases: {v.nlargest(6).sum():,.0f}',
                                      'ha': 'right', 'size': 8, 'family': 'DejaVu Sans'},
    perpendicular_bar_func='median',
    period_length=500,
    figsize=(5, 3),
    dpi=144,
    cmap='dark12',
    title='COVID-19 impacts in USA',
    title_size='',
    bar_label_size=7,
    tick_label_size=7,
    shared_fontdict={'family' : 'DejaVu Sans', 'color' : '.1'},
    scale='linear',
    writer=None,
    fig=None,
    bar_kwargs={'alpha': .7},
    filter_column_colors=True)

"""Initially, New York was the most confirmed cases, later in july august - california. Curretly Texaz is the top state to have most confirmed case in Covid-19

What are most affected states by Covid-19?
"""

import numpy as np
import matplotlib.pyplot as plt 

state_covid = confirmed_grp_df1.groupby('province_state', as_index=False).agg({"confirmed": "sum"}) #Sum covid deaths based on states
state_covid_top = state_covid.nlargest(5,'confirmed') #Pick top 5 states

def func(pct, allvalues): 
    absolute = int(pct / 100.*np.sum(allvalues)) 
    return "{:.1f}\n({:d} #cnt)".format(pct, absolute) 

fig = plt.figure(figsize =(10, 7)) 
ax = fig.add_axes([0,0,1,1])
ax.axis('equal')

ax.pie(state_covid_top.confirmed, labels = state_covid_top.province_state, autopct = lambda pct: func(pct, state_covid_top.confirmed)) 

ax.legend(state_covid_top.province_state, 
          title ="Top state", 
          loc ="center left", 
          bbox_to_anchor =(1, 0, 0.5, 1)) 

ax.set_title("Top-Five States in USA - Covid19") 

plt.show()

import pandas as pd

df2 = pd.read_csv("https://storage.googleapis.com/analytics3/time_series_covid19_deaths_US.csv")
df2.head(5)

df2.columns = df2.columns.str.lower() #Change all the column names to lowercase
df2.head(3)

df2.columns = df2.columns.str.replace('long_', 'long') #Change column name from "long_" to "long"
df2.describe()

df2.drop(['uid','iso2',"iso3", "code3", "country_region"] , axis=1 , inplace=True) #To the Copied data frame drop all columns other than date values

df2.dropna(subset=["fips"], inplace=True) #Drop null values in fips, county code cannot be NULL
df2.describe()

df2['fips'] = df2['fips'].astype(int).astype(str) #Convert fips datatype from float to string
print(df2.head(4))

df2['fips'] = df2['fips'].map(lambda x: f'{x:0>5}') #Append 0 in the front of fips column to have length of 5
print(df2.head(4))

i = df2[ df2.population == 0 ].index #Find index of records, where population is equal to 0
df2.drop(i, inplace=True) #Since population can not be 0 & county codes is not valid too for these records, drop all the records, where population is equal to 0.
df2.describe()

#df2_sum = df2.copy(deep=True) #Copy the data frame to another dataframe to sum all the date values
#df2_sum.drop(["fips", "admin2", "lat", "long", "combined_key", "population"] , axis=1 , inplace=True) #To the Copied data frame drop all columns other than date values
#df2_sum.drop(list(df2_sum.filter(regex='^(?!1.*/20)')), axis=1 , inplace=True)
#df2_sum.head()

dates = df2.columns[7:]
death_df2 = df2.melt(
    id_vars=['province_state'], 
    value_vars=dates, 
    var_name='date', 
    value_name='death'
)
#death_df2['date'] = pd.to_datetime(death_df2['date'], format='%m/%d/%Y')
death_grp_df2 = death_df2.groupby(['province_state']).sum('death').reset_index()
death_grp_df2.head(5)

df2_date = df2.copy(deep=True) #Copy the data frame to another dataframe to sum all the date values
df2_date.drop(["fips", "admin2", "province_state", 
                               "lat", "long", "combined_key", "population"] , axis=1 , inplace=True) #To the Copied data frame drop all columns other than date values
df2['death_covid'] = df2_date.sum(axis = 1, skipna = True) #Sum all the date column values and add it to the initial dataframe as a new column
df2['death_percentage'] = (df2['death_covid']/df2['population'])*100
df2[ (df2.fips == '06001')] #Check value for a county in California

"""How is the death rate in USA?"""

import plotly
from urllib.request import urlopen
import json
with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:
    counties = json.load(response)

import pandas as pd
import plotly.express as px
import plotly.graph_objs as go

fig = px.choropleth_mapbox(df2, geojson=counties, locations='fips', color='death_percentage',
                             color_continuous_scale="Viridis",
                             title =("2020 Covid19 Death Rate by County") ,
                             range_color=(0, 10),
                             mapbox_style="carto-positron",
                             zoom=3, center = {"lat": 37.0902, "lon": -95.7129},
                             opacity=0.5,
                             labels={'death_in_percentage':'Death Rate'})

fig.show()

"""We can see, south of USA has more Covud death percentage."""

df2_sum = df2.copy(deep=True)
df2_sum.drop(["fips", "admin2", "lat", "long", "combined_key", "population"] , axis=1 , inplace=True) 
df2_sum['jan_d'] = df2_sum[list(df2_sum.filter(regex='^1/.*/20'))].sum(axis = 1, skipna = True)
df2_sum['feb_d'] = df2_sum[list(df2_sum.filter(regex='^2.*/20'))].sum(axis = 1, skipna = True)
df2_sum['mar_d'] = df2_sum[list(df2_sum.filter(regex='^3.*/20'))].sum(axis = 1, skipna = True)
df2_sum['apr_d'] = df2_sum[list(df2_sum.filter(regex='^4.*/20'))].sum(axis = 1, skipna = True)
df2_sum['may_d'] = df2_sum[list(df2_sum.filter(regex='^5.*/20'))].sum(axis = 1, skipna = True)
df2_sum['jun_d'] = df2_sum[list(df2_sum.filter(regex='^6.*/20'))].sum(axis = 1, skipna = True)
df2_sum['jul_d'] = df2_sum[list(df2_sum.filter(regex='^7.*/20'))].sum(axis = 1, skipna = True)
df2_sum['aug_d'] = df2_sum[list(df2_sum.filter(regex='^8.*/20'))].sum(axis = 1, skipna = True)
df2_sum['sep_d'] = df2_sum[list(df2_sum.filter(regex='^9.*/20'))].sum(axis = 1, skipna = True)
df2_sum['oct_d'] = df2_sum[list(df2_sum.filter(regex='^10.*/20'))].sum(axis = 1, skipna = True)
df2_sum['nov_d'] = df2_sum[list(df2_sum.filter(regex='^11.*/20'))].sum(axis = 1, skipna = True)
df2_sum.drop(list(df2_sum.filter(regex='.*/20')) , axis=1 , inplace=True) 

df2_grp_state = df2_sum.groupby(['province_state']).sum('jan_d').reset_index()

df2_grp_state

#df2_sum.drop(list(df2_sum.filter(regex='^(?!1.*/20) & ^(?!province_state)')), axis=1 , inplace=True)
#df2_sum[ (df2_sum.province_state == 'California')]

"""How Covid-19 Confirmed and Death cases affected each state?"""

import altair as alt

chart = alt.Chart(confirmed_grp_df1)

conf_line = alt.Chart(confirmed_grp_df1).mark_line().encode(
    alt.X('province_state:N'),
    alt.Y('confirmed:Q'))

conf_point = alt.Chart(confirmed_grp_df1).mark_circle().encode(
    alt.X('province_state:N'),
    alt.Y('confirmed:Q'), tooltip=['province_state', 'confirmed'])

red = alt.value('#f54242')
brush = alt.selection_interval()

death_line = alt.Chart(death_grp_df2).mark_line().encode(
    alt.X('province_state:N'),
    alt.Y('death:Q'), color=red)

death_point = alt.Chart(death_grp_df2).mark_circle().encode(
    alt.X('province_state:N'),
    alt.Y('death:Q'),  color=red, tooltip=['province_state', 'death'], 
    size=alt.Size('death',
        scale=alt.Scale(range=[0, 1000])))


conf_line + conf_point + death_line + death_point

"""California has the highest count in confirmed cases whereas New York is the top state affected by Covid Deaths.

What are most affected states by Covid-19 deaths?
"""

import numpy as np
import matplotlib.pyplot as plt 

state_covid = death_grp_df2.groupby('province_state', as_index=False).agg({"death": "sum"}) #Sum covid deaths based on states
state_covid_top = state_covid.nlargest(5,'death') #Pick top 5 states

def func(pct, allvalues): 
    absolute = int(pct / 100.*np.sum(allvalues)) 
    return "{:.1f}\n({:d} #cnt)".format(pct, absolute) 

fig = plt.figure(figsize =(10, 7)) 
ax = fig.add_axes([0,0,1,1])
ax.axis('equal')

ax.pie(state_covid_top.death, labels = state_covid_top.province_state, autopct = lambda pct: func(pct, state_covid_top.death)) 

ax.legend(state_covid_top.province_state, 
          title ="Top state", 
          loc ="center left", 
          bbox_to_anchor =(1, 0, 0.5, 1)) 

ax.set_title("Top-Five States in USA - Covid19 Death") 

plt.show()



"""What is most affected counties in USA?"""

import matplotlib.pyplot as plt 
from matplotlib.pyplot import figure

county_covid = df2.groupby('combined_key', as_index=False).agg({"death_covid": "sum"}) #Sum covid deaths based on county, states and country
county_covid_top = county_covid.nlargest(5,'death_covid') #Select top 5 Counties

df23 = pd.DataFrame() 

for col in county_covid_top.combined_key: #For each county fetch the records from existing dataframe and create new dataframe
  county_data = df2[df2.combined_key == col ]
  df23 = df23.append(county_data)

df23.drop(["fips", "admin2", "province_state", "lat", "long", 
                    "population", "death_covid", "death_percentage"] , axis=1 , inplace=True) #To the Copied data frame drop all columns other than date and combined_key column
df23.drop(list(df23.filter(regex='^2|^1.*/20')), axis=1 , inplace=True) 

df25 = df23.pivot_table( columns=['combined_key']) #Pivot the dataframe

df25.plot(kind='line', figsize=(8,5))
plt.title("Covid19 Death trend of Top 5 Counties of USA")
plt.xlabel('Increase in covid values by date', fontsize=10)
plt.ylabel('Covid count', fontsize='medium')
plt.legend(bbox_to_anchor=(1.1, 1.05))
plt.xticks(ticks=None, rotation=30)
plt.Formatter()

"""Out of five, 3 are from New York

How is the death rate compared to the population?
"""

dates = df2.columns[7:]
death_pop_df2 = df2.melt(
    id_vars=['province_state', 'population'], 
    value_vars=dates, 
    var_name='date', 
    value_name='death'
)
death_pop_grp_df2 = death_pop_df2.groupby(['province_state']).sum('death').reset_index()

race_state_chart = death_pop_grp_df2[death_pop_grp_df2['province_state'].isin(['New York', 'New Jersey', 'California', 'Florida', 'Texas'])]
race_state_chart['death_percentage'] = (race_state_chart['death']/race_state_chart['population'])*100
race_state_chart

alt.Chart(race_state_chart).mark_bar().encode(
    x='population:Q',
    y='death_percentage:Q',
    color='province_state:N',
tooltip=['population', 'death', 'province_state'] # show Name and Origin in a tooltip
).interactive()

"""The above chart shows, death rate is more in New York and New Jersey"""

df1_sum = df1.copy(deep=True)
df1_sum.drop(["fips", "uid", "admin2", "lat", "long", "combined_key"] , axis=1 , inplace=True) 
df1_sum['jan_c'] = df1_sum[list(df1_sum.filter(regex='^1/.*/20'))].sum(axis = 1, skipna = True)
df1_sum['feb_c'] = df1_sum[list(df1_sum.filter(regex='^2.*/20'))].sum(axis = 1, skipna = True)
df1_sum['mar_c'] = df1_sum[list(df1_sum.filter(regex='^3.*/20'))].sum(axis = 1, skipna = True)
df1_sum['apr_c'] = df1_sum[list(df1_sum.filter(regex='^4.*/20'))].sum(axis = 1, skipna = True)
df1_sum['may_c'] = df1_sum[list(df1_sum.filter(regex='^5.*/20'))].sum(axis = 1, skipna = True)
df1_sum['jun_c'] = df1_sum[list(df1_sum.filter(regex='^6.*/20'))].sum(axis = 1, skipna = True)
df1_sum['jul_c'] = df1_sum[list(df1_sum.filter(regex='^7.*/20'))].sum(axis = 1, skipna = True)
df1_sum['aug_c'] = df1_sum[list(df1_sum.filter(regex='^8.*/20'))].sum(axis = 1, skipna = True)
df1_sum['sep_c'] = df1_sum[list(df1_sum.filter(regex='^9.*/20'))].sum(axis = 1, skipna = True)
df1_sum['oct_c'] = df1_sum[list(df1_sum.filter(regex='^10.*/20'))].sum(axis = 1, skipna = True)
df1_sum['nov_c'] = df1_sum[list(df1_sum.filter(regex='^11.*/20'))].sum(axis = 1, skipna = True)
df1_sum.drop(list(df1_sum.filter(regex='.*/20')) , axis=1 , inplace=True) 

df1_grp_state = df1_sum.groupby(['province_state']).sum('jan_c').reset_index()

df1_grp_state

c_d_merge = pd.merge(left=df1_grp_state, right=df2_grp_state, how='outer', left_on='province_state', right_on='province_state')
c_d_merge.head(5)

c_d_grp_merge = pd.merge(left=confirmed_grp_df1, right=death_grp_df2, how='outer', left_on='province_state', right_on='province_state') ######Can be removed
c_d_grp_merge

#Isida

import pandas as pd

race_df = pd.read_csv("https://storage.googleapis.com/isidandreu_project1/Provisional_COVID-19_Death_Counts_by_County_and_Race.csv")
race_df.head(5)

race_df.columns = race_df.columns.str.lower()
race_df.info()

race_col = race_df.columns.str.replace('\s+', '_')
race_df.columns = race_col
race_df.head(5)

race_df['non-hispanic_white'] = race_df['non-hispanic_white'].fillna(0)
race_df['non-hispanic_black'] = race_df['non-hispanic_black'].fillna(0)
race_df['non-hispanic_american_indian_or_alaska_native'] = race_df['non-hispanic_american_indian_or_alaska_native'].fillna(0)
race_df['non-hispanic_asian'] = race_df['non-hispanic_asian'].fillna(0)
race_df['non-hispanic_native_hawaiian_or_other_pacific_islander'] = race_df['non-hispanic_native_hawaiian_or_other_pacific_islander'].fillna(0)
race_df['hispanic'] = race_df['hispanic'].fillna(0)
race_df['other'] = race_df['other'].fillna(0)
race_df.head(4)

race_df.drop(['data_as_of','start_week','end_week', 'urban_rural_code',	'fips_state',	'fips_county','footnote'] , axis=1 , inplace=True)
race_df['fips_code'] = race_df['fips_code'].astype(int).astype(str) #Convert fips datatype from float to string
race_df['fips_code'] = race_df['fips_code'].map(lambda x: f'{x:0>5}')
race_df['indicator'] = race_df['indicator'].astype(str) 
race_df.head(4)

df_melted = race_df.melt(id_vars=['fips_code','state','county_name','indicator','total_deaths','covid-19_deaths'], value_vars=['non-hispanic_white',	'non-hispanic_black',	'non-hispanic_american_indian_or_alaska_native',	'non-hispanic_asian',	'non-hispanic_native_hawaiian_or_other_pacific_islander',	'hispanic',	'other'], value_name='percentage', var_name='race')
df_melted.head(5)

population_filter = race_df['indicator'].str.contains('population', case=False, na=False)
total_death_filter = race_df['indicator'].str.contains('all-cause deaths', case=False, na=False)
covid19_death_filter = race_df['indicator'].str.contains('COVID-19 deaths', case=False, na=False)

population_filter.sum()

total_death_filter.sum()

covid19_death_filter.sum()

population_df = race_df[population_filter]

population_df.drop(['indicator'] , axis=1 , inplace=True)
population_df.head(4)

"""Which area is most affected?"""

urban_rural = population_df.groupby('urban_rural_description')['covid-19_deaths'].sum()
pie = urban_rural.plot.pie(colors=['#abd6ce', '#FFCA33', '#FF5733', '#ffeaac', '#db9bae'], 
                                     autopct=(lambda p : '{:.2f}%'.format(p)),
                                     fontsize=10, 
                                     figsize=(8, 8))
pie.yaxis.set_label_coords(-0.15, 0.5)
pie.set_title('Covid-19 Deaths by Areas')

"""Large central metro is affected(New York is the most affected)."""

total_death_df = race_df[total_death_filter]
total_death_df.drop(['indicator'] , axis=1 , inplace=True)
total_death_df.head(4)

covid19_death_df = race_df[covid19_death_filter]
covid19_death_df.drop(['indicator'] , axis=1 , inplace=True)
covid19_death_df.head(4)

df_merge = pd.merge(left=population_df, right=total_death_df, how='inner', left_on='fips_code', right_on='fips_code')
df_merge.head(5)

df_merge.drop(['state_y','county_name_y','total_deaths_y', 'covid-19_deaths_y',	'urban_rural_description_y'] , axis=1 , inplace=True)
df_merge.head(2)

race_df_final = pd.merge(left=df_merge, right=covid19_death_df, how='inner', left_on='fips_code', right_on='fips_code')
race_df_final.head(2)

race_df_final.drop(['state','county_name','total_deaths', 'covid-19_deaths',	'urban_rural_description_x'] , axis=1 , inplace=True)
race_df_final.head(2)

race_df_final.rename(columns = {'non-hispanic_white_x':'non-hispanic_white_population'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_black_x':'non-hispanic_black_population'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_american_indian_or_alaska_native_x':'non-hispanic_american_indian_or_alaska_native_population'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_asian_x':'non-hispanic_asian_population'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_native_hawaiian_or_other_pacific_islander_x':'non-hispanic_native_hawaiian_or_other_pacific_islander_population'}, inplace = True) 
race_df_final.rename(columns = {'hispanic_x':'hispanic_population'}, inplace = True) 
race_df_final.rename(columns = {'other_x':'other_population'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_white_y':'non-hispanic_white_total_death'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_black_y':'non-hispanic_black_total_death'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_american_indian_or_alaska_native_y':'non-hispanic_american_indian_or_alaska_native_total_death'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_asian_y':'non-hispanic_asian_total_death'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_native_hawaiian_or_other_pacific_islander_y':'non-hispanic_native_hawaiian_or_other_pacific_islander_total_death'}, inplace = True) 
race_df_final.rename(columns = {'hispanic_y':'hispanic_total_death'}, inplace = True) 
race_df_final.rename(columns = {'other_y':'other_total_death'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_white':'non-hispanic_white_covid19_death'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_black':'non-hispanic_black_covid19_death'}, inplace = True) 
race_df_final.rename(columns = {'non-hispanic_american_indian_or_alaska_native':'non-hispanic_american_indian_or_alaska_native_covid19_death'}, inplace = True)
race_df_final.rename(columns = {'non-hispanic_asian':'non-hispanic_asian_covid19_death'}, inplace = True)
race_df_final.rename(columns = {'non-hispanic_native_hawaiian_or_other_pacific_islander':'non-hispanic_native_hawaiian_or_other_pacific_islander_covid19_death'}, inplace = True)
race_df_final.rename(columns = {'hispanic':'hispanic_covid19_death'}, inplace = True)
race_df_final.rename(columns = {'other':'other_covid19_death'}, inplace = True)
race_df_final.rename(columns = {'state_x':'state'}, inplace = True)
race_df_final.rename(columns = {'county_name_x':'county_name'}, inplace = True)
race_df_final.rename(columns = {'total_deaths_x':'total_deaths'}, inplace = True)
race_df_final.rename(columns = {'covid-19_deaths_x':'covid-19_deaths'}, inplace = True)
race_df_final.rename(columns = {'fips_code':'fips'}, inplace = True)
race_df_final.head(2)

age_gender_df = pd.read_csv("https://storage.googleapis.com/isidandreu_project1/Provisional_COVID-19_Death_Counts_by_Sex__Age__and_State_updated.csv")
age_gender_df.head(2)

age_gender_df.columns = age_gender_df.columns.str.lower()

age_gender_df.info()

age_gender_df.state.describe()

age_gender_col = age_gender_df.columns.str.replace('\s+', '_')
age_gender_df.columns = age_gender_col
age_gender_df.head(5)

age_gender_df.drop(['data_as_of','start_week','end_week', 'pneumonia_deaths',	'pneumonia_and_covid-19_deaths', 'influenza_deaths', 'pneumonia,_influenza,_or_covid-19_deaths', 'footnote'] , axis=1 , inplace=True)
age_gender_df.head(2)

age_gender_df['covid-19_deaths'] = age_gender_df['covid-19_deaths'].fillna(0)
age_gender_df['total_deaths'] = age_gender_df['total_deaths'].fillna(0)
age_gender_df.head(2)

race_df_final.head(2)

df2_state = df2.loc[:, ['fips', 'province_state']]
df2_state.head(2)

df2_state.info()

#race_aggregated = pd.merge(left=race_df_final_aggregated_sum, right=race_df_final_aggregated_avg, how='inner', left_on='state', right_on='state')
#race_aggregated.head(2)

race_state = pd.merge(left=race_df_final, right=df2_state, how='inner', left_on='fips', right_on='fips')
race_state.head(2)

race_state.head(5)

print(race_df_final.info())
print(race_state.info())

race_df_final_aggregated = race_state.loc[:, ['state', 'province_state', 'total_deaths',	'covid-19_deaths',	'non-hispanic_white_population',	'non-hispanic_black_population',	'non-hispanic_american_indian_or_alaska_native_population',	'non-hispanic_asian_population',	'non-hispanic_native_hawaiian_or_other_pacific_islander_population',	'hispanic_population',	'other_population',	'non-hispanic_white_total_death',	'non-hispanic_black_total_death',	'non-hispanic_american_indian_or_alaska_native_total_death',	'non-hispanic_asian_total_death',	'non-hispanic_native_hawaiian_or_other_pacific_islander_total_death',	'hispanic_total_death',	'other_total_death',	'non-hispanic_white_covid19_death',	'non-hispanic_black_covid19_death',	'non-hispanic_american_indian_or_alaska_native_covid19_death',	'non-hispanic_asian_covid19_death',	'non-hispanic_native_hawaiian_or_other_pacific_islander_covid19_death',	'hispanic_covid19_death',	'other_covid19_death']]
race_df_final_aggregated.head(2)

race_df_final_aggregated_sum_groups = race_df_final_aggregated.groupby(['province_state'])['total_deaths',	'covid-19_deaths'].sum()
race_df_final_aggregated_sum = pd.DataFrame(race_df_final_aggregated_sum_groups)
race_df_final_aggregated_sum.head(2)

race_df_final_aggregated_avg_group = race_df_final_aggregated.groupby(['province_state'])['non-hispanic_white_population',	'non-hispanic_black_population',	'non-hispanic_american_indian_or_alaska_native_population',	'non-hispanic_asian_population',	'non-hispanic_native_hawaiian_or_other_pacific_islander_population',	'hispanic_population',	'other_population',	'non-hispanic_white_total_death',	'non-hispanic_black_total_death',	'non-hispanic_american_indian_or_alaska_native_total_death',	'non-hispanic_asian_total_death',	'non-hispanic_native_hawaiian_or_other_pacific_islander_total_death',	'hispanic_total_death',	'other_total_death',	'non-hispanic_white_covid19_death',	'non-hispanic_black_covid19_death',	'non-hispanic_american_indian_or_alaska_native_covid19_death',	'non-hispanic_asian_covid19_death',	'non-hispanic_native_hawaiian_or_other_pacific_islander_covid19_death',	'hispanic_covid19_death',	'other_covid19_death'].mean()
race_df_final_aggregated_avg = pd.DataFrame(race_df_final_aggregated_avg_group)
race_df_final_aggregated_avg.head(2)

race_aggregated_merge = pd.merge(right=race_df_final_aggregated_avg, left=race_df_final_aggregated_sum, how='inner', left_on='province_state', right_on='province_state')
race_aggregated_merge.head(2)

race_aggregated_merge.info()

age_gender_df.head(2)

age_gender_df.info()

race_state.head(2)

df2_st_pop = df2.loc[:, ['fips', 'province_state', 'population']]

race_state_pop = pd.merge(left=race_df_final, right=df2_st_pop, how='inner', left_on='fips', right_on='fips')
race_state_pop.head(2)

total_by_state_10 = race_aggregated_merge.nlargest(10,'covid-19_deaths')
total_by_state_10

covid19_deaths_by_age_group = age_gender_df.groupby('age_group')['covid-19_deaths', 'total_deaths'].mean()
sub = covid19_deaths_by_age_group.plot.bar( title='Total and Covid-19 Deaths Rate by Age Group')
sub.set_ylabel('Covid-19 Death Rate')

unstacked_age_gender = age_gender_df.groupby(['age_group', 'sex'])['covid-19_deaths'].mean().unstack()
unstacked_age_gender.plot(kind='bar', stacked=True)

pop_filter = df_melted['indicator'].str.contains('population', case=False, na=False)
death_filter = df_melted['indicator'].str.contains('all-cause deaths', case=False, na=False)
covid_death = df_melted['indicator'].str.contains('COVID-19 deaths', case=False, na=False)

df_melted_population = df_melted[pop_filter]
df_melted_population.drop(['indicator'] , axis=1 , inplace=True)

df_melted_deaths = df_melted[death_filter]
df_melted_deaths.drop(['indicator'] , axis=1 , inplace=True)

df_melted_covid_deaths = df_melted[covid_death]
df_melted_covid_deaths.drop(['indicator'] , axis=1 , inplace=True)

print(df_melted_population.head(2))
print(df_melted_deaths.head(2))
df_melted_covid_deaths.head(2)

race_melted_aggregated_merge = pd.merge(left=df_melted_population, right=df_melted_deaths, how='inner', left_on=['fips_code','county_name','race'], right_on=['fips_code','county_name','race'])
race_melted_aggregated_merge.drop(['state_y',	'total_deaths_y',	'covid-19_deaths_y'] , axis=1 , inplace=True) 
race_melted_aggregated_merge.rename(columns = {'state_x':'state'}, inplace = True) 
race_melted_aggregated_merge.rename(columns = {'total_deaths_x':'total_deaths'}, inplace = True) 
race_melted_aggregated_merge.rename(columns = {'covid-19_deaths_x':'covid-19_deaths'}, inplace = True) 
race_melted_aggregated_merge.rename(columns = {'percentage_x':'percentage_of_distributed_population'}, inplace = True) 
race_melted_aggregated_merge.rename(columns = {'percentage_y':'percentage_of_total_deaths'}, inplace = True)
race_melted_aggregated_merge_final = pd.merge(left=race_melted_aggregated_merge, right=df_melted_covid_deaths, how='inner', left_on=['fips_code','county_name','race'], right_on=['fips_code','county_name','race'])  
race_melted_aggregated_merge_final.drop(['state_y',	'total_deaths_y',	'covid-19_deaths_y'] , axis=1 , inplace=True)
race_melted_aggregated_merge_final.rename(columns = {'state_x':'state'}, inplace = True) 
race_melted_aggregated_merge_final.rename(columns = {'total_deaths_x':'total_deaths'}, inplace = True) 
race_melted_aggregated_merge_final.rename(columns = {'covid-19_deaths_x':'covid-19_deaths'}, inplace = True) 
race_melted_aggregated_merge_final.rename(columns = {'percentage':'percentage_of_covid-19_deaths'}, inplace = True)
race_melted_aggregated_merge_final.head()

test = race_melted_aggregated_merge_final.groupby(['state','race'])['percentage_of_covid-19_deaths'].mean()
print(test)

# importing the required library 
import pandas as pd 
import seaborn as sns 
import matplotlib.pyplot as plt 
race_melted_aggregated_merge_final['distributed_population'] = race_melted_aggregated_merge_final['percentage_of_distributed_population']*100
df_melted_population['percentage_of_distributed_population'] = df_melted_population['percentage']*100
sns.lmplot(x ='covid-19_deaths', y ='percentage_of_distributed_population',  
           fit_reg = False, hue = 'race', 
          data = df_melted_population) 
  
# Show the plot 
plt.show()

race_melted_aggregated_merge_final['covid-19_deaths_by_race'] = race_melted_aggregated_merge_final['percentage_of_covid-19_deaths']* race_melted_aggregated_merge_final['covid-19_deaths']
race_melted_aggregated_merge_final['total_deaths_by_race'] = race_melted_aggregated_merge_final['percentage_of_total_deaths']* race_melted_aggregated_merge_final['total_deaths']
#pd.to_numeric(race_melted_aggregated_merge_final['covid-19_deaths_by_race'], downcast='integer')
#pd.to_numeric(race_melted_aggregated_merge_final['total_deaths_by_race'], downcast='integer')
race_melted_aggregated_merge_final['total_deaths_by_race'] = race_melted_aggregated_merge_final['total_deaths_by_race'].astype(int)
race_melted_aggregated_merge_final['covid-19_deaths_by_race'] = race_melted_aggregated_merge_final['covid-19_deaths_by_race'].astype(int)
covid19_total_death = race_melted_aggregated_merge_final.groupby('race')['total_deaths_by_race','covid-19_deaths_by_race'].mean()

sub = covid19_total_death.plot.bar( title='Total Deaths & Covid-19 Deaths per Race')
sub.set_ylabel('Total/Covid-19 Deaths')
#race_melted_aggregated_merge_final.head(5)

#Reading the dataset
Airlines_Monthly_Data = pd.read_csv("https://storage.googleapis.com/tgundapaneni/Airlines_Monthly_Data_1.csv")

#Displaying top 5 rows
Airlines_Monthly_Data.head()

#Change all the column names to lowercase
Airlines_Monthly_Data.columns = Airlines_Monthly_Data.columns.str.lower()

#Replacing space with _
Airlines_Monthly_Data.columns = Airlines_Monthly_Data.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')

#Replacing ,
Airlines_Monthly_Data = Airlines_Monthly_Data.replace(',','', regex=True)

#Displaying top 5 rows
Airlines_Monthly_Data.head()

#Dispalying information on columns
Airlines_Monthly_Data.info()

#Changing the datatype of columns to int
Airlines_Monthly_Data["total_scheduled_domestic_flights"] = Airlines_Monthly_Data["total_scheduled_domestic_flights"].astype(int)
Airlines_Monthly_Data["total_domestic_flights_operated"] = Airlines_Monthly_Data["total_domestic_flights_operated"].astype(int)
Airlines_Monthly_Data["total_domestic_flight_cancellations"] = Airlines_Monthly_Data["total_domestic_flight_cancellations"].astype(int)
Airlines_Monthly_Data["total_domestic_flights_operated"] = Airlines_Monthly_Data["total_domestic_flights_operated"].astype(int)
Airlines_Monthly_Data["alaska_scheduled_df"] = Airlines_Monthly_Data["alaska_scheduled_df"].astype(int)
Airlines_Monthly_Data["alaska_operated_df"] = Airlines_Monthly_Data["alaska_operated_df"].astype(int)
Airlines_Monthly_Data["alaska_cancelled_df"] = Airlines_Monthly_Data["alaska_cancelled_df"].astype(int)
Airlines_Monthly_Data["allegiant_scheduled_df"] = Airlines_Monthly_Data["allegiant_scheduled_df"].astype(int)
Airlines_Monthly_Data["allegiant_operated_df"] = Airlines_Monthly_Data["allegiant_operated_df"].astype(int)
Airlines_Monthly_Data["allegiant_cancelled_df"] = Airlines_Monthly_Data["allegiant_cancelled_df"].astype(int)
Airlines_Monthly_Data["american_scheduled_df"] = Airlines_Monthly_Data["american_scheduled_df"].astype(int)
Airlines_Monthly_Data["american_operated_df"] = Airlines_Monthly_Data["american_operated_df"].astype(int)
Airlines_Monthly_Data["american_cancelled_df"] = Airlines_Monthly_Data["american_cancelled_df"].astype(int)
Airlines_Monthly_Data["delta_scheduled_df"] = Airlines_Monthly_Data["delta_scheduled_df"].astype(int)
Airlines_Monthly_Data["delta_operated_df"] = Airlines_Monthly_Data["delta_operated_df"].astype(int)
Airlines_Monthly_Data["delta_cancelled_df"] = Airlines_Monthly_Data["delta_cancelled_df"].astype(int)
Airlines_Monthly_Data["frontier_scheduled_df"] = Airlines_Monthly_Data["frontier_scheduled_df"].astype(int)
Airlines_Monthly_Data["frontier_operated_df"] = Airlines_Monthly_Data["frontier_operated_df"].astype(int)
Airlines_Monthly_Data["frontier_cancelled_df"] = Airlines_Monthly_Data["frontier_cancelled_df"].astype(int)
Airlines_Monthly_Data["hawaiian_scheduled_df"] = Airlines_Monthly_Data["hawaiian_scheduled_df"].astype(int)
Airlines_Monthly_Data["hawaiian_operated_df"] = Airlines_Monthly_Data["hawaiian_operated_df"].astype(int)
Airlines_Monthly_Data["hawaiian_cancelled_df"] = Airlines_Monthly_Data["hawaiian_cancelled_df"].astype(int)
Airlines_Monthly_Data["jetblue_scheduled_df"] = Airlines_Monthly_Data["jetblue_scheduled_df"].astype(int)
Airlines_Monthly_Data["jetblue_operated_df"] = Airlines_Monthly_Data["jetblue_operated_df"].astype(int)
Airlines_Monthly_Data["jetblue_cancelled_df"] = Airlines_Monthly_Data["jetblue_cancelled_df"].astype(int)
Airlines_Monthly_Data["southwest_scheduled_df"] = Airlines_Monthly_Data["southwest_scheduled_df"].astype(int)
Airlines_Monthly_Data["southwest_operated_df"] = Airlines_Monthly_Data["southwest_operated_df"].astype(int)
Airlines_Monthly_Data["southwest_cancelled_df"] = Airlines_Monthly_Data["southwest_cancelled_df"].astype(int)
Airlines_Monthly_Data["spirit_scheduled_df"] = Airlines_Monthly_Data["spirit_scheduled_df"].astype(int)
Airlines_Monthly_Data["spirit_operated_df"] = Airlines_Monthly_Data["spirit_operated_df"].astype(int)
Airlines_Monthly_Data["spirit_cancelled_df"] = Airlines_Monthly_Data["spirit_cancelled_df"].astype(int)
Airlines_Monthly_Data["united_scheduled_df"] = Airlines_Monthly_Data["united_scheduled_df"].astype(int)
Airlines_Monthly_Data["united_operated_df"] = Airlines_Monthly_Data["united_operated_df"].astype(int)
Airlines_Monthly_Data["united_cancelled_df"] = Airlines_Monthly_Data["united_cancelled_df"].astype(int)

#Displaying info about columns
Airlines_Monthly_Data.info()

#Displaying data
Airlines_Monthly_Data.head(5)

#Defining the new dataset idx which shows us the maximum date in the month
idx = pd.date_range(start='2019-01', freq='M', periods=19)
idx

#Inserting idx as a column into our dataset
Airlines_Monthly_Data.insert(2,"daterange",idx,True)

#Displaying our data
Airlines_Monthly_Data.head(5)

Airlines_Monthly_Data.info()

#Dividing the datasets based on year. Using grouping functions
grouped = Airlines_Monthly_Data.groupby(Airlines_Monthly_Data.year) 
grouped
Airlines_Monthly_Data_2019 = grouped.get_group(2019) 
Airlines_Monthly_Data_2019.head(5)

#Year 2020 data
Airlines_Monthly_Data_2020 = grouped.get_group(2020) 
Airlines_Monthly_Data_2020

#@title
#Forming new dataset from Airlines_Monthly_Data_2019
Airlines_Monthly_Data_2019_OA = pd.DataFrame(Airlines_Monthly_Data_2019, columns = ['daterange', 'total_scheduled_domestic_flights', 'total_domestic_flights_operated', 'total_domestic_flight_cancellations']) 
Airlines_Monthly_Data_2019_OA

#Forming new dataset from Airlines_Monthly_Data_2020
Airlines_Monthly_Data_2020_OA = pd.DataFrame(Airlines_Monthly_Data_2020, columns = ['daterange', 'total_scheduled_domestic_flights', 'total_domestic_flights_operated', 'total_domestic_flight_cancellations']) 
Airlines_Monthly_Data_2020_OA

# convert the datetime column to a datetime type and assign it back to the column
Airlines_Monthly_Data_2019_OA.daterange = pd.to_datetime(Airlines_Monthly_Data_2019_OA.daterange)
display(Airlines_Monthly_Data_2019_OA.head())

#TimeSeries Plot

import matplotlib.pyplot as plt 
from matplotlib.pyplot import figure

fig, (ax1, ax2) = plt.subplots(1, 2)

plt.title("Flight pattern of 2020")
plt.ylabel('No of Flights', fontsize='medium')

Airlines_Monthly_Data_2019_OA.plot(x='daterange', ax=ax1, figsize=(15,5), xlabel='Months 2019', ylabel='No of Flights', title='Flight pattern of 2019')
plt.legend(bbox_to_anchor=(1.1, 1.05))

Airlines_Monthly_Data_2020_OA.plot(x='daterange', ax=ax2, xlabel='Months 2020')
plt.legend(bbox_to_anchor=(1.1, 1.00))
plt.tight_layout()

df2_grp_state1 = df2_grp_state.drop(columns=['death_covid','death_percentage'])
df2_grp_state1.head(5)

df2_grp_state_transposed = df2_grp_state1.T
df2_grp_state_transposed["sum"] = df2_grp_state_transposed.sum(axis=1)
df2_grp_state_transposed

df2_grp_state_transposed_wp=df2_grp_state_transposed.drop(df2_grp_state_transposed.index[0])
df2_grp_state_transposed_wp

idx1 = pd.date_range(start='2020-01', freq='M', periods=11)
idx1
df2_grp_state_transposed_wp.insert(2,"daterange",idx1,True)

df2_grp_state_transposed_wp_sum = df2_grp_state_transposed_wp[['daterange','sum']]
df2_grp_state_transposed_wp_sum

#Creating a new dataset by merging two datasets
tm_ds = pd.DataFrame(Airlines_Monthly_Data_2020_OA, columns = ['daterange', 'total_scheduled_domestic_flights', 'total_domestic_flights_operated', 'total_domestic_flight_cancellations']) 
tm_ds

df2_grp_state_transposed_wp_sum_dropped = df2_grp_state_transposed_wp_sum
df2_grp_state_transposed_wp_sum_dropped
df2_grp_state_transposed_wp_sum_dropped = df2_grp_state_transposed_wp_sum_dropped.drop(['aug_d', 'sep_d', 'oct_d', 'nov_d']) 
df2_grp_state_transposed_wp_sum_dropped
df2_grp_state_transposed_wp_sum_dropped.columns = df2_grp_state_transposed_wp_sum_dropped.columns.str.replace('sum', 'total_number_of_covid_Cases')

#tm_ds.insert(4, "total_number_of_covid_cases",df2_grp_state_transposed_wp_sum_dropped.sum , True) 
#tm_ds
tm_ds = pd.merge(tm_ds, df2_grp_state_transposed_wp_sum_dropped)
tm_ds

import matplotlib.pyplot as plt 
from matplotlib.pyplot import figure

fig, (ax1, ax2, ax3) = plt.subplots(1, 3)

plt.title("Domestic Air Travel pattern of 2020")
plt.ylabel('No of Flights', fontsize='medium')

Airlines_Monthly_Data_2019_OA.plot(x='daterange', ax=ax1, figsize=(20,5), xlabel='Months 2019', ylabel='No of Flights', title='Domestic Air Travel pattern of 2019')
plt.legend(bbox_to_anchor=(1.1, 1.05))

tm_ds.plot(x='daterange', y='total_number_of_covid_Cases', ax=ax2, xlabel='Months 2020', ylabel='Covid cases', title='Covid Pattern over the months')
plt.legend(bbox_to_anchor=(1.1, 1.05))

tm_ds.plot(x='daterange',y=["total_scheduled_domestic_flights", "total_domestic_flights_operated", "total_domestic_flight_cancellations"], ax=ax3, xlabel='Months 2020')
plt.legend(bbox_to_anchor=(1.1, 1.00))
plt.tight_layout()

Airlines_employment = pd.read_excel('https://storage.googleapis.com/tgundapaneni/Airlines_Employment.xlsx', parse_dates=['Month'])
Airlines_employment.head(5)

Airlines_employment.columns = Airlines_employment.columns.str.lower()
Airlines_employment.columns = Airlines_employment.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')
Airlines_employment.head(5)

Airlines_employment.dtypes

from datetime import datetime
Airlines_employment['month'] = pd.to_datetime(Airlines_employment['month'], format='%m/%d/%y') #Change the date values to date format
#Airlines_employment = datetime.strptime(Airlines_employment.month, '%m-%d-%Y').date()
#print(type(date_object))
#print(date_object)
Airlines_employment.head(5)

df10 = Airlines_employment.copy(deep=True) #Copy the data frame to another dataframe to sum all the date values

df10.drop(['full_time','part_time'] , axis=1 , inplace=True) #To the Copied data frame drop all columns other than date values

df11 = df10.groupby(['carrier_name', 'month']).sum('total').reset_index() #Group by carrier_name and month to sum the total for each carrier_name/month
df11.head(5)

df_alaska = df11[df11['carrier_name'] == "Alaska"] 
df_allegiant = df11[df11['carrier_name'] == "Allegiant Air"] 
df_american = df11[df11['carrier_name'] == "American"] 
df_delta = df11[df11['carrier_name'] == "Delta"] 
df_frontier = df11[df11['carrier_name'] == "Frontier"] 
df_hawaiian = df11[df11['carrier_name'] == "Hawaiian"] 
df_jetblue = df11[df11['carrier_name'] == "JetBlue"] 
df_southwest = df11[df11['carrier_name'] == "Southwest"] 
df_spirit = df11[df11['carrier_name'] == "Spirit"] 
df_united = df11[df11['carrier_name'] == "United"]

df_alaska['difference'] = df_alaska["total"].diff(1)
df_allegiant['difference'] = df_allegiant["total"].diff(1)
df_american['difference'] = df_american["total"].diff(1)
df_delta['difference'] = df_delta["total"].diff(1)
df_frontier['difference'] = df_frontier["total"].diff(1)
df_hawaiian['difference'] = df_hawaiian["total"].diff(1)
df_jetblue['difference'] = df_jetblue["total"].diff(1)
df_southwest['difference'] = df_southwest["total"].diff(1)
df_spirit['difference'] = df_spirit["total"].diff(1)
df_united['difference'] = df_united["total"].diff(1)

df_alaska['difference'] = df_alaska['difference'].fillna(0)
df_allegiant['difference'] = df_allegiant['difference'].fillna(0)
df_american['difference'] = df_american['difference'].fillna(0)
df_delta['difference'] = df_delta['difference'].fillna(0)
df_frontier['difference'] = df_frontier['difference'].fillna(0)
df_hawaiian['difference'] = df_hawaiian['difference'].fillna(0)
df_jetblue['difference'] = df_jetblue['difference'].fillna(0)
df_southwest['difference'] = df_southwest['difference'].fillna(0)
df_spirit['difference'] = df_spirit['difference'].fillna(0)
df_united['difference'] = df_united['difference'].fillna(0)

df_alaska

df_merged = pd.DataFrame()
df_merged

df_merged = df_alaska
df_merged

df_merged.append(df_allegiant, ignore_index = True) 
df_merged

df_merged = pd.concat([df_merged, df_allegiant])
df_merged = pd.concat([df_merged, df_american])
df_merged = pd.concat([df_merged, df_delta])
df_merged = pd.concat([df_merged, df_frontier])
df_merged = pd.concat([df_merged, df_hawaiian])
df_merged = pd.concat([df_merged, df_jetblue])
df_merged = pd.concat([df_merged, df_southwest])
df_merged = pd.concat([df_merged, df_spirit])
df_merged = pd.concat([df_merged, df_united])
df_merged

df_only_difference = df_merged
del df_only_difference['total']
df_only_difference

df13 = df_only_difference.pivot_table(index=['month'], columns=['carrier_name']) #Convert carrier_name as rows to columns
df13

import bar_chart_race as bcr

#df10 = Airlines_employment.copy(deep=True) #Copy the data frame to another dataframe to sum all the date values

#df4.drop(["fips", "admin2", "lat", "long", "combined_key", "population"] , axis=1 , inplace=True) #To the Copied data frame drop all columns other than date values

#cols=df4.filter(regex=':*/20').columns #Fetch all the date column names

#df5 = df4.melt(id_vars=['province_state'], value_vars=(cols), value_name='count', var_name='date') #Convert date columns to rows
#df11 = df10.groupby(['carrier_name', 'month']).sum('full_time','part_time').reset_index() #Group by carrier_name and month to sum the total for each carrier_name/month
#df11['month'] = pd.to_datetime(df11['month'], format='%Y/%m/%d %H:%M:%S.%f') #Change the date values to date format
#df7 = df6.pivot_table(index=['month'], columns=['carrier_name']) #Convert carrier_name as rows to columns

bcr.bar_chart_race(
    df=df13,
    filename=None,
    orientation='h',
    sort='desc',
    n_bars=10,
    fixed_order=False,
    fixed_max=True,
    steps_per_period=10,
    interpolate_period=False,
    label_bars=True,
    bar_size=.95,
    period_label={'x': .99, 'y': .25, 'ha': 'right', 'va': 'center'},
    period_fmt='%B %d, %Y',
    period_summary_func=lambda v, r: {'x': .99, 'y': .18,
                                      's': f'Count of Employees: {v.nlargest(6).sum():,.0f}',
                                      'ha': 'right', 'size': 8, 'family': 'DejaVu Sans'},
    perpendicular_bar_func='median',
    period_length=500,
    figsize=(5, 3),
    dpi=144,
    cmap='dark12',
    title='Number of Employees laid off by 10 Airlines',
    title_size='',
    bar_label_size=7,
    tick_label_size=7,
    shared_fontdict={'family' : 'DejaVu Sans', 'color' : '.1'},
    scale='linear',
    writer=None,
    fig=None,
    bar_kwargs={'alpha': 1},
    filter_column_colors=True)

unemploy = pd.read_excel("https://storage.googleapis.com/tgundapaneni/County%20Unemployment%20Data.xlsx")

unemploy.head(5)

unemploy.columns = unemploy.columns.str.lower()
unemploy.info()

unemploy['county'], unemploy['state'] = unemploy['area_title'].str.split(',', 1).str
unemploy.head()

unemploy['county1'] = unemploy['county'].str.replace(' County', '')
unemploy['county2'] = unemploy['county1'].str.strip()
unemploy.head()

unemploy['county2'].str.len()

unemploy.drop(['laus_area_code','county','area_title','county1'] , axis=1 , inplace=True)
unemploy.head()

unemploy['county2'] = unemploy['county2'].str.replace('Borough', '')
unemploy['county2'] = unemploy['county2'].str.strip()
unemploy.head()

unemploy['state'] = unemploy['state'].str.strip()
unemploy[unemploy.state == 'AK']

unemploy['county2'] = unemploy['county2'].str.replace('Census Area', '')
unemploy['county2'] = unemploy['county2'].str.replace('/city', '')
unemploy['county2'] = unemploy['county2'].str.replace('/municipality', '')
unemploy['county2'] = unemploy['county2'].str.replace('Municipality', '')
unemploy['county2'] = unemploy['county2'].str.strip()
unemploy[unemploy.state == 'AK']

unemploy['civilian_labor_force'] = unemploy.civilian_labor_force.astype(str).str.replace('-', '0')
unemploy['employed'] = unemploy.employed.astype(str).str.replace('-', '0')
unemploy['unemployment_level'] = unemploy.unemployment_level.astype(str).str.replace('-', '0')
unemploy['unemployment_rate'] = unemploy.unemployment_rate.astype(str).str.replace('-', '0')
unemploy[unemploy.state == 'PR']

unemploy["civilian_labor_force"] = unemploy["civilian_labor_force"].astype(int)
unemploy["employed"] = unemploy["employed"].astype(int)
unemploy["unemployment_level"] = unemploy["unemployment_level"].astype(int)
unemploy["unemployment_rate"] = unemploy["unemployment_rate"].astype(float)
unemploy['period'] = unemploy['period'].str.strip()
unemploy_mi = unemploy[(unemploy.state == 'MI')]
unemploy_mi_apr = unemploy_mi[(unemploy_mi.period == 'Apr-20')]
unemploy_mi_apr

d = {'civilian_labor_force': 'sum', 'employed': 'sum','unemployment_level': 'sum','unemployment_rate': 'mean'}
unemploy1 = unemploy.groupby(['state','period'], as_index=False).aggregate(d).reindex(columns=unemploy.columns)
unemploy1[unemploy1.state=='PR']

unemploy1 = unemploy1.drop(columns='county2')
unemploy1[unemploy1.state=='PR']

unemploy1[unemploy1.civilian_labor_force== 0]

unemploy1['civilian_labor_force'].replace(to_replace=0, method='bfill',inplace=True)
unemploy1['employed'].replace(to_replace=0, method='bfill',inplace=True)
unemploy1['unemployment_level'].replace(to_replace=0, method='bfill',inplace=True)
unemploy1['unemployment_rate'].replace(to_replace=0, method='bfill',inplace=True)
unemploy1[unemploy1.state=='PR']

unemploy1.columns = ['postal_code','period','civilian_labor_force',	'employed',	'unemployment_level',	'unemployment_rate']
unemploy1.head(15)

unemploy1['period'] = unemploy1['period'].str.replace(r"\(.*?\)","")
unemploy1['period'] = unemploy1['period'].str.strip()
unemploy19 = unemploy1[unemploy1['period'].str.contains('19')]
unemploy19['period'] = unemploy19['period'].str.replace(r"\(.*?\)","")
unemploy19['period'] = unemploy19['period'].str.strip()
unemploy2 = unemploy1[unemploy1['period'].str.contains('20')] 
unemploy2['period'] = unemploy2['period'].str.replace(r"\(.*?\)","")
unemploy2['period'] = unemploy2['period'].str.strip()
unemploy2.head(15)

unemploy_mi = unemploy2[(unemploy2.postal_code == 'MI')]
unemploy_mi_apr = unemploy_mi[(unemploy_mi.period == 'Apr-20')]
unemploy_mi_apr

Final_pay = pd.read_excel("https://storage.googleapis.com/tgundapaneni/Final_Pay.xlsx")
Final_pay.head()

State_code = pd.read_excel ("https://storage.googleapis.com/tgundapaneni/US_StateCodes.xlsx")
State_code.head()

State_code.columns = ['State', 'postal_code']
State_code.head()

Final_state = pd.merge(State_code,Final_pay)
Final_state.head()

Final_state.columns = ['state','postal_code','Mar-20','Apr-20','May-20','Jun-20','Jul-20','Aug-20','Sep-20']
Final_state.head()

Final_state_merge= Final_state.melt(id_vars=['postal_code','state'], value_vars=('Mar-20','Apr-20','May-20','Jun-20','Jul-20','Aug-20','Sep-20'), value_name='claims_count', var_name='period') 
Final_state_merge.head(5)

unemploy_claim = pd.merge(Final_state_merge,unemploy2, how='inner', left_on=['postal_code','period'], right_on = ['postal_code','period'])
unemploy_claim.head()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

unemploy_claim_grp = unemploy_claim.groupby(['state']).mean('unemployment_rate').reset_index() 

plt.figure(figsize=(15,10))
ax = sns.barplot(x="state", y="unemployment_rate", data=unemploy_claim_grp)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize = 10)
ax.set(xlabel='USA states', ylabel='Unemployment Rate')
ax.set_title("Statewise - Unemployment Rate USA")

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import dates
import seaborn as sns

#unemploy_claim_prd = unemploy_claim.groupby('period', as_index=False).mean({"unemployment_rate": "sum"}) 
#unemploy_claim_prd_iloc = pd.DataFrame() 

#unemploy_claim_prd_iloc = unemploy_claim_prd_iloc.append(unemploy_claim_prd.loc[4])
#unemploy_claim_prd_iloc = unemploy_claim_prd_iloc.append(unemploy_claim_prd.loc[0])
#unemploy_claim_prd_iloc = unemploy_claim_prd_iloc.append(unemploy_claim_prd.loc[5])
#unemploy_claim_prd_iloc = unemploy_claim_prd_iloc.append(unemploy_claim_prd.loc[3])
#unemploy_claim_prd_iloc = unemploy_claim_prd_iloc.append(unemploy_claim_prd.loc[2])
#unemploy_claim_prd_iloc = unemploy_claim_prd_iloc.append(unemploy_claim_prd.loc[1])
#unemploy_claim_prd_iloc = unemploy_claim_prd_iloc.append(unemploy_claim_prd.loc[6])


unemploy_claim_cali_19 = unemploy1[(unemploy1.postal_code == 'CA')].copy(deep=True).reset_index() 

unemploy_claim_cali_19_il = pd.DataFrame()
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[1])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[12])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[11])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[10])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[3])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[5])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[4])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[8])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[0])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[9])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[7])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[6])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[2])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[13])


ax = sns.lineplot(data=unemploy_claim_cali_19_il, x='period', y='unemployment_rate', marker="o")
ax.set(xlabel='Unemplyment Period', ylabel='Unemplyment Rate')
ax.set(title="Unemployment Rate in USA")

import plotly.express as px # Be sure to import express


unemploy_claim_apr = unemploy_claim[(unemploy_claim.period == 'Apr-20')].copy(deep=True) 

fig = px.choropleth(unemploy_claim_apr,  # Input Pandas DataFrame
                    locations="postal_code",  # DataFrame column with locations
                    color="unemployment_rate",  # DataFrame column with color values
                    hover_name="postal_code", # DataFrame column hover info
                    locationmode = 'USA-states',
                    color_continuous_scale="Viridis",
                    range_color=(0, 10)) # Set to plot as US States
fig.update_layout(
    title_text = 'Apr 2020 - Unemployment Rate in USA', # Create a Title
    geo_scope='usa',  # Plot only the USA instead of globe
)
fig.show()

df1_grp_state_cpy = df1_grp_state.copy(deep=True)
#df1_grp_state_cpy = df1_grp_state_cpy.set_index('province_state') 

df1_grp_state_cpy.plot(x='province_state', y=['mar_c','apr_c', 'sep_c'], kind='line', figsize=(15,10))
plt.title("Covid Trend in Mar, Apr and Nov 2020")
plt.xlabel('US States', fontsize=10)
plt.ylabel('Covid trend over states', fontsize='medium')
plt.legend(bbox_to_anchor=(1.1, 1.05))
plt.legend(["March Covid - Trend", "April Covid - Trend", 'September Covid - Trend'])
plt.Formatter()

import matplotlib.pyplot as plt 
from matplotlib.pyplot import figure



unemploy_claim_cali_19 = unemploy1[(unemploy1.postal_code == 'CA')].copy(deep=True).reset_index() 

unemploy_claim_cali_19_il = pd.DataFrame()
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[1])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[12])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[11])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[10])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[3])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[5])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[4])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[8])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[0])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[9])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[7])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[6])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[2])
unemploy_claim_cali_19_il = unemploy_claim_cali_19_il.append(unemploy_claim_cali_19.loc[13])

unemploy_claim_cali = unemploy_claim[(unemploy_claim.state == 'California')].copy(deep=True)

df2_cali = df2[(df2.province_state == 'California')].copy(deep=True)
df2_cali.drop(["fips", "admin2", "lat", "long", "combined_key", "population", "death_percentage", "death_covid"] , axis=1 , inplace=True) 
cols=df2_cali.filter(regex=':*/20').columns #Fetch all the date column names
df2_cali_melt = df2_cali.melt(id_vars=['province_state'], value_vars=(cols), value_name='count', var_name='date')

fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)

#unemploy_claim_cali.plot(x='period', y='unemployment_rate', ax=ax1, figsize=(15,5), xlabel='Period', ylabel='Unemployment Rate', title='California trend in Unemployment 2020')

unemploy_claim_cali_19_il.plot(x='period', y='unemployment_rate', ax=ax1, figsize=(15,5), xlabel='Period', ylabel='Unemployment Rate', title='California trend in Unemployment 2019')

unemploy_claim_cali.plot(x='period', y='claims_count', ax=ax2, xlabel='Period', ylabel='Unemployment claims', title='California - No of Received Claims')

unemploy_claim_cali_19_il.plot(x='period', y=['employed','unemployment_level'], ax=ax3, xlabel='Period', ylabel='Emplyed vs Unemployed', title='California trend in Employed/Unemployed')

df2_cali_melt.plot(x='date', y='count', ax=ax4, xlabel='Period', ylabel='Covid Deaths', title='California trend Covid deaths')

plt.tight_layout()